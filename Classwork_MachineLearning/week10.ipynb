{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Astronomy 406 \"Computational Astrophysics\" (Fall 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 10: Multimodality of a distribution, Expectation-Maximization, Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Reading:</b> notes below, as well as $\\S$16.1 of <a href=\"http://www.nr.com\">Numerical Recipes</a>, and $\\S$4.4, 6.3.1 of <a href=\"http://www.astroml.org/\">Machine Learning</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"savefig.dpi\"] = 80\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating and interpreting confidence limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa, bb, ss = np.loadtxt(\"bootstrap.dat\", unpack=True)\n",
    "\n",
    "def kde_tophat( data, x, h ):\n",
    "    y = (abs(x - data[:,None]) <= h).astype(float)\n",
    "    return y.sum(0)/(2*h*len(data))\n",
    "\n",
    "def conf_limit( x, lim ):\n",
    "    dx = np.sort(np.abs(x-np.mean(x)))\n",
    "    return dx[int(lim*len(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.mean(bb)\n",
    "b1 = conf_limit(bb, 0.683)\n",
    "b2 = conf_limit(bb, 0.954)\n",
    "b3 = conf_limit(bb, 0.997)\n",
    "\n",
    "print 'db: at 68.3 = %.3f  at 95.4 = %.3f  at 99.7 = %.3f' % (b1, b2, b3)\n",
    "\n",
    "x = np.linspace(1.3, 1.6, 1000)\n",
    "plt.plot(x, kde_tophat(bb, x, 0.02))\n",
    "x = np.linspace(b-b1, b+b1, 200)\n",
    "plt.fill_between(x, kde_tophat(bb, x, 0.02), 0, alpha=0.3)\n",
    "plt.xlim(1.3, 1.57)\n",
    "plt.xlabel('b')\n",
    "plt.ylabel('N')\n",
    "\n",
    "plt.plot([b, b], [0, 11], '--r')\n",
    "plt.plot([b-b1, b-b1], [0, 7], '-k')\n",
    "plt.plot([b+b1, b+b1], [0, 7.2], '-k')\n",
    "plt.plot([b-b2, b-b2], [0, 1.7], '-k')\n",
    "plt.plot([b+b2, b+b2], [0, 1.6], '-k')\n",
    "plt.plot([b-b3, b-b3], [0, 0.3], '-k')\n",
    "plt.plot([b+b3, b+b3], [0, 0.2], '-k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split an observed distribution into a sum of Gaussian modes.  The probability of observing a point $x_n$ is\n",
    "\n",
    "$P(x_n) = \\sum_{k=1}^{K} p_k N(x_n | \\mu_k,\\sigma_k)$\n",
    "\n",
    "where $p_k$ are the weights of each mode $k$.  The likelihood of observing the whole data set is\n",
    "\n",
    "${\\cal L} = \\prod_n P(x_n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.loadtxt(\"DataFiles/gc.dat\", usecols=(0,), unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = [-1, 0]\n",
    "sig = [1., 1.]\n",
    "pk = [0.5, 0.5]\n",
    "\n",
    "Kmodes = len(mu)\n",
    "logL = 999.\n",
    "logL1 = 99.\n",
    "iter = 0\n",
    "pnk = np.zeros([Kmodes, len(x)])\n",
    "\n",
    "while abs(logL-logL1) > 1.e-6:\n",
    "    # E-step\n",
    "    iter += 1\n",
    "    pxn = np.zeros(len(x))\n",
    "    for k in xrange(Kmodes):\n",
    "        pxn += pk[k]/np.sqrt(2.*np.pi)/sig[k]*np.exp(-((x-mu[k])/sig[k])**2/2.)\n",
    "\n",
    "    logL1 = logL\n",
    "    logL = np.sum(np.log(pxn))\n",
    "\n",
    "    # M-step\n",
    "    for k in xrange(Kmodes):     \n",
    "        pnk[k] = pk[k]/np.sqrt(2.*np.pi)/sig[k]*np.exp(-((x-mu[k])/sig[k])**2/2.)/pxn\n",
    "\n",
    "    for k in xrange(Kmodes):\n",
    "        w = pnk[k]/sig[k]**2\n",
    "        sig2k = np.average((x-mu[k])**2, weights=w)\n",
    "        sig[k] = np.sqrt(sig2k)\n",
    "        mu[k] = np.average(x, weights=w)\n",
    "        pk[k] = np.average(pnk[k])\n",
    "        \n",
    "print logL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = 0.1\n",
    "xmin = -2.5\n",
    "xmax = 0.5\n",
    "bins = np.arange(xmin, xmax, d)\n",
    "xx = np.linspace(xmin, xmax, num=100)\n",
    "yy = np.zeros(len(xx))\n",
    "for k in xrange(len(mu)):\n",
    "    yy += len(x)*d*pk[k]/np.sqrt(2.*np.pi)/sig[k]\\\n",
    "          *np.exp(-((xx-mu[k])/sig[k])**2/2.)    \n",
    "        \n",
    "plt.xlabel('x')\n",
    "plt.ylabel('N')\n",
    "plt.hist(x, bins)\n",
    "plt.plot(xx, yy, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we choose the right number of modes to describe our data set?  It is a non-trivial problem, which is a part of general problem of <b>model selection</b>.  We consider some simple rules first.\n",
    "\n",
    "The Aikake information criterion (derived from information theory):\n",
    "\n",
    "$\\mathrm{AIC} \\equiv -2\\ln{{\\cal L}_{max}} + 2K + {2K (K+1) \\over N-K-1}$.\n",
    "\n",
    "The Bayesian information criterion:\n",
    "\n",
    "$\\mathrm{BIC} \\equiv -2\\ln{{\\cal L}_{max}} + K \\ln{N}$.\n",
    "\n",
    "In principle, the likelihood ${\\cal L}$ can always be improved by increasing the number of free parameters.  Both criteria penalize the likelihood for number of model parameters $K$.  The most appropriate model would minimize AIC and BIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 1:</b> Choose the right number of modes for our data set based on AIC and BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GMM_simple(mu, sig, pk, x):\n",
    "    ...\n",
    "    return logL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the GMM algorithm provided by <b>sklearn</b> package and take examples of <a href=\"http://www.astroml.org/book_figures/chapter4/fig_GMM_1D.html\">Figure 4.2</a> and <a href=\"http://www.astroml.org/book_figures/chapter6/fig_EM_metallicity.html\">Figure 6.6</a> in ML.\n",
    "\n",
    "Read also <a href=\"http://scikit-learn.org/stable/modules/mixture.html\">useful information</a> on the implementation of GMM in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GMM\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = np.arange(1, 6)\n",
    "models = [None for i in K]\n",
    "\n",
    "X = x.reshape(-1,1)\n",
    "\n",
    "for i in range(len(K)):\n",
    "    models[i] = GMM(K[i], random_state=1, n_init=10).fit(X)\n",
    "\n",
    "AIC = [m.aic(X) for m in models]\n",
    "BIC = [m.bic(X) for m in models]\n",
    "print \"AIC=\", AIC[:4]\n",
    "print \"BIC=\", BIC[:4]\n",
    "\n",
    "M_best = models[np.argmin(AIC)]\n",
    "\n",
    "d = 0.2\n",
    "xmin = -2.5\n",
    "xmax = 0.5\n",
    "bins = np.arange(xmin, xmax, d)\n",
    "xx = np.linspace(xmin, xmax, num=100)\n",
    "yy = np.zeros(len(xx))\n",
    "\n",
    "logprob = M_best.score_samples(X)\n",
    "pdf = np.exp(logprob[:][0])\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "\n",
    "plt.hist(x, bins, normed=True, histtype='stepfilled', alpha=0.4)\n",
    "plt.plot(xx, pdf, '-k')\n",
    "plt.plot(xx, pdf_individual, '--k')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p(x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(K, AIC, 'r-', label='AIC')\n",
    "plt.plot(K, BIC, 'b--', label='BIC')\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('information criterion')\n",
    "plt.legend(loc=2, frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feh, m, d = np.loadtxt(\"DataFiles/gc.dat\", unpack=True)\n",
    "logd = np.log10(d)\n",
    "X = np.vstack([feh, logd]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = np.arange(1, 10)\n",
    "models = [None for n in N]\n",
    "\n",
    "for i in range(len(N)):\n",
    "    models[i] = GMM(N[i], covariance_type='full').fit(X)\n",
    "\n",
    "AIC = [m.aic(X) for m in models]\n",
    "BIC = [m.bic(X) for m in models]\n",
    "\n",
    "i_best = np.argmin(BIC)\n",
    "gmm_best = models[i_best]\n",
    "print \"best fit converged:\", gmm_best.converged_\n",
    "print \"BIC: N components = %i\" % N[i_best]\n",
    "\n",
    "plt.plot(N, AIC, 'r-', label='AIC')\n",
    "plt.plot(N, BIC, 'b--', label='BIC')\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('information criterion')\n",
    "plt.legend(loc=2, frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from astroML.plotting.tools import draw_ellipse\n",
    "\n",
    "plt.scatter(feh, logd)\n",
    "plt.xlabel('[Fe/H]')\n",
    "plt.ylabel('log d (kpc)')\n",
    "\n",
    "for mu, C, w in zip(gmm_best.means_, gmm_best.covars_, gmm_best.weights_):\n",
    "    draw_ellipse(mu, C, scales=[2], fc='none', ec='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Exercise 2:</b> Compare the GMM model parameters and information criteria obtained with the explicit method above and with the sklearn package, for the univariable dataset feh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
