{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Astronomy 406 \"Computational Astrophysics\" (Fall 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 12: Machine Learning methods for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Reading:</b> notes below, as well as $\\S$9.3-9.7 of [Machine Learning](http://www.astroml.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week we looked at [Gaussian Mixture](http://scikit-learn.org/stable/modules/mixture.html) modeling. It can be used to assign data points to distinct and separate classes.  [**Sklearn**](http://scikit-learn.org) (or Scikit-Learn) module provides several others methods for classification:\n",
    "\n",
    "[Nearest Neighbors](http://scikit-learn.org/stable/modules/neighbors.html)\n",
    "\n",
    "[Support Vector Machines](http://scikit-learn.org/stable/modules/svm.html)\n",
    "\n",
    "[Random Forest](http://scikit-learn.org/stable/modules/ensemble.html#forest)\n",
    "\n",
    "A handy comparison of all different classification methods is given [here](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html).\n",
    "\n",
    "We will use examples provided by ML.<br>\n",
    "Gaussian classifier:\n",
    "[Figure 9.1](http://www.astroml.org/book_figures/chapter9/fig_bayes_DB.html) and\n",
    "[Figure 9.2](http://www.astroml.org/book_figures/chapter9/fig_simple_naivebayes.html)<br>\n",
    "SVM classifier:\n",
    "[Figure 9.9](http://www.astroml.org/book_figures/chapter9/fig_svm_diagram.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"savefig.dpi\"] = 90\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, svm, ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SDSS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a large dataset of galaxies with spectroscopic information from the SDSS survey. This survey obtained images for 357,000,000 stars and galaxies in five photometric bands, as well as spectroscopy for 1,600,000 galaxies, quasars, and stars. AstroML provides access to a subset of this huge information. It is described [here](http://www.astroml.org/user_guide/datasets.html). To load the properties of galaxies from the spectroscopic dataset, execute the following cell. The output lists various fields listed in the dataset, including galaxy positions on the sky, redshift, velocity dispersion, magnitudes in the SDSS $u$, $g$, $r$, $i$, and $z$ bands, stellar mass, star formation rate, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from astroML.datasets import fetch_sdss_specgals\n",
    "from astroML.plotting import scatter_contour\n",
    "from astroML.plotting.tools import draw_ellipse\n",
    "\n",
    "data = fetch_sdss_specgals()\n",
    "print data.shape, 'spectra fields:', data.dtype.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let make a common color-magnitude diagram, for the $g-r$ color vs. $r$-band magnitude. To make a cleaner dataset, we restrict the redshift range to $0.02 < z < 0.06$. This reduces the number of objects from 661598 to 114527, still a huge number for a statistical study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# redshift cut \n",
    "data = data[data['z'] > 0.02]\n",
    "data = data[data['z'] < 0.06]\n",
    "\n",
    "gr = data['modelMag_g'] - data['modelMag_r']\n",
    "r = data['modelMag_r']\n",
    "print len(r), 'galaxies selected'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "scatter_contour(r, gr, threshold=400, log_counts=True, ax=ax,\n",
    "                histogram2d_args=dict(bins=100),\n",
    "                plot_args=dict(marker=',', linestyle='none', color='black'),\n",
    "                contour_args=dict(cmap=plt.cm.bone))\n",
    "\n",
    "ax.set_xlabel(r'${\\rm r}$', size=16)\n",
    "ax.set_ylabel(r'${\\rm g - r}$', size=16)\n",
    "ax.set_xlim(18, 14)\n",
    "ax.set_ylim(0, 1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bimodality in the color distribution. The long narrow band around $g-r = 0.8$ is called *red sequence* galaxies, while the big cloud at $0.3 < g-r < 0.5$ is the *blue cloud* (star-forming) galaxies. The histogram below confirms this bimodal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(gr, bins=np.arange(0.2,1,0.02), histtype='step')\n",
    "plt.xlabel(r'${\\rm g - r}$', size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, from a much smaller dataset, it would be difficult to infer this bimodality. Let's take the first hundred galaxies and plot their color histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(gr[:100], bins=np.arange(0.2,1,0.02), histtype='step')\n",
    "plt.xlabel(r'${\\rm g - r}$', size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning methods allow efficient exploration of datasets.  They are often divided into the categories of **Supervised** and **Unsupervised** methods.\n",
    "\n",
    "**Supervised** methods generally deal with classification of objects. They are given a **labeled** training dataset, and the model is later applied to **un-labeled** data in order to predict the unknown label.\n",
    "\n",
    "**Unsupervised** methods generally deal with clustering of objects or density estimation. They are given an **un-labeled** training dataset, and make inferences about the structure of the data without any label input. One familiar example is Kernel Density Estimator (KDE) which we used instead of histograms. Another example is Gaussian Mixture Modeling.\n",
    "\n",
    "Most models in **Sklearn** have similar syntax. They are trained on a particular dataset using the ``fit()`` method. Then labels for new points can be predicted using the ``predict()`` method. This makes it very convenient to try different Machine Learning models just by changing the initialization step.\n",
    "\n",
    "**Sklearn** also provides a routine (``cross_val_score``) for cross-validation of the model parameters, that is, evaluation of how well a particular model could be expected to perform on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of selecting the training set and the test set. Let's take 100 and 1000 points from our galaxy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.vstack([r, gr]).T\n",
    "print 'total dataset:', X.shape, 'galaxies'\n",
    "\n",
    "Xtraining = X[:100]\n",
    "print 'training set:', len(Xtraining), 'galaxies'\n",
    "\n",
    "Xtest = X[-1000:]\n",
    "print 'test set:', len(Xtest), 'galaxies'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin our exploration of this dataset with unsupervised Gaussian Mixture Modeling, which does not require the knowledge of any labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "N = np.arange(1, 6)\n",
    "models = [None for n in N]\n",
    "\n",
    "for i in range(len(N)):\n",
    "    models[i] = GaussianMixture(N[i], covariance_type='full').fit(Xtraining)\n",
    "\n",
    "AIC = [m.aic(Xtraining) for m in models]\n",
    "BIC = [m.bic(Xtraining) for m in models]\n",
    "\n",
    "i_best = np.argmin(BIC)\n",
    "gmm_best = models[i_best]\n",
    "print 'best fit converged:', gmm_best.converged_\n",
    "print 'number of interations =', gmm_best.n_iter_\n",
    "print 'BIC: N components = %i' % N[i_best]\n",
    "\n",
    "plt.plot(N, AIC, 'r-', label='AIC')\n",
    "plt.plot(N, BIC, 'b--', label='BIC')\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('information criterion')\n",
    "plt.legend(loc=2, frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both information criteria prefer three modes. Let's color the points according to the predicted split of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmap_bold = ListedColormap(['#FF0000', '#0000FF', '#00FF00',])\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF', '#AAFFAA'])\n",
    "\n",
    "gmm_best = models[2]\n",
    "\n",
    "plt.scatter(Xtraining[:,0], Xtraining[:,1], c=gmm_best.predict(Xtraining), cmap=cmap_bold, alpha=0.7, s=6)\n",
    "plt.xlim(18,14)\n",
    "plt.ylim(0,1.2)\n",
    "plt.xlabel(r'${\\rm r}$', size=16)\n",
    "plt.ylabel(r'${\\rm g - r}$', size=16)\n",
    "\n",
    "for mu, C, w in zip(gmm_best.means_, gmm_best.covariances_, gmm_best.weights_):\n",
    "    draw_ellipse(mu, C, scales=[2], fc='none', ec='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can predict the expected mode for any value of $r, g-r$ and color the whole space according to the predicted mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(18,14,-0.01), np.arange(0,1.2,0.01))\n",
    "\n",
    "Z = gmm_best.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=.8, cmap=cmap_light)\n",
    "plt.xlim(18,14)\n",
    "plt.ylim(0,1.2)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Nearest Neighbors](http://scikit-learn.org/stable/modules/neighbors.html) method could be as supervised or unsupervised.\n",
    "\n",
    "Let's use the supervised version and assign the training labels two values (True or False) based on whether $g-r > 0.65$, which seems like a reasonable split by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "border = 0.65\n",
    "target = (Xtraining[:,1] > border)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize the Sklearn's implementation of the [NN Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), evaluate fit for the training dataset, and predict labels for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = neighbors.KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
    "\n",
    "# evaluate fit \n",
    "clf.fit(Xtraining, target)\n",
    "\n",
    "# apply fit to the test data\n",
    "y = clf.predict(Xtest)\n",
    "\n",
    "plt.scatter(Xtest[:,0], Xtest[:,1], c=y, cmap=cmap_bold, alpha=0.7, s=6)\n",
    "plt.axhline(border)\n",
    "plt.xlim(18,14)\n",
    "plt.ylim(0,1.2)\n",
    "plt.xlabel(r'${\\rm r}$', size=16)\n",
    "plt.ylabel(r'${\\rm g - r}$', size=16)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close-up view near the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(Xtest[:,0], Xtest[:,1], c=y, cmap=cmap_bold, alpha=0.7, s=6)\n",
    "plt.axhline(border)\n",
    "plt.xlim(18,14)\n",
    "plt.ylim(0.45,0.85)\n",
    "plt.xlabel(r'${\\rm r}$', size=16)\n",
    "plt.ylabel(r'${\\rm g - r}$', size=16)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a colored map of predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(18,14,-0.02), np.arange(0,1.2,0.02))\n",
    "\n",
    "if hasattr(clf, \"decision_function\"):\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "else:\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=.8, cmap=cmap_light)\n",
    "plt.xlim(18,14)\n",
    "plt.ylim(0.4,0.9)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Support vector machines](http://scikit-learn.org/stable/modules/svm.html) (SVMs) are a set of supervised learning methods. SVMs draw a boundary between clusters of data, which maximizes the perpendicular distance between the clusters.\n",
    "\n",
    "Sklearn implementation of the [SVM Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) has many options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(gamma=2)\n",
    "\n",
    "# evaluate fit \n",
    "clf.fit(Xtraining, target)\n",
    "\n",
    "# apply fit to the test data\n",
    "y = clf.predict(Xtest)\n",
    "\n",
    "plt.scatter(Xtest[:,0], Xtest[:,1], c=y, cmap=cmap_bold, alpha=0.7, s=6)\n",
    "plt.axhline(border)\n",
    "plt.xlim(18,14)\n",
    "plt.ylim(0,1.2)\n",
    "plt.xlabel(r'${\\rm r}$', size=16)\n",
    "plt.ylabel(r'${\\rm g - r}$', size=16)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Random Forest](http://scikit-learn.org/stable/modules/ensemble.html#forest) is a type of the decision tree algorithm.\n",
    "\n",
    "Sklearn implementation of the [RF Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = ensemble.RandomForestClassifier()\n",
    "\n",
    "# evaluate fit \n",
    "clf.fit(Xtraining, target)\n",
    "\n",
    "# apply fit to the test data\n",
    "y = clf.predict(Xtest)\n",
    "\n",
    "plt.scatter(Xtest[:,0], Xtest[:,1], c=y, cmap=cmap_bold, alpha=0.7, s=6)\n",
    "plt.axhline(border)\n",
    "plt.xlim(18,14)\n",
    "plt.ylim(0,1.2)\n",
    "plt.xlabel(r'${\\rm r}$', size=16)\n",
    "plt.ylabel(r'${\\rm g - r}$', size=16)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
